{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Web Scraping Jobs Web-sites</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Required Libraries & Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the List for the Columns of Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Title_Name = []\n",
    "Company_Name = []\n",
    "Experience_Required = []\n",
    "Salary = []\n",
    "Location = []\n",
    "Job_Description = []\n",
    "Key_Skills = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Scraping Times Jobs Web-site</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 21):\n",
    "    url = \"https://www.timesjobs.com/candidate/job-search.html?from=submit&actualTxtKeywords=AI%20Engineer&searchBy=0&rdoOperator=OR&searchType=personalizedSearch&txtLocation=India&luceneResultSize=25&postWeek=60&txtKeywords=ai%20engineer&pDate=I&sequence=\" + \\\n",
    "        str(i) + \"&startPage=1\"\n",
    "    # print(url)\n",
    "    sleep(randint(2, 10))\n",
    "\n",
    "    # Making a GET request\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # check status code for response received\n",
    "    # success code - 200\n",
    "    # print(r)\n",
    "\n",
    "    # print content of request\n",
    "    htmlContent = r.content\n",
    "    # print(htmlContent)\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    # print(soup.prettify)\n",
    "\n",
    "    # HTML Tree Traversal\n",
    "\n",
    "    # Get the Title of the HTML page\n",
    "    title = soup.title\n",
    "    # print(title.string)\n",
    "\n",
    "    Job_Title = soup.find_all('h2')\n",
    "\n",
    "    for h2 in Job_Title:\n",
    "        A = h2.get_text()\n",
    "        t1 = re.split(r'\\n\\n|\\n\\r\\n', A)\n",
    "        Job_Title_Name.append(t1[1])\n",
    "\n",
    "    # print(Job_Title_Name)\n",
    "\n",
    "    Company = soup.find_all('h3')\n",
    "    Tem = []\n",
    "\n",
    "    for Company_name in Company:\n",
    "        C_Name = Company_name.get_text()\n",
    "        t1 = C_Name.split('\\r\\n')\n",
    "\n",
    "        Tem.append(t1)\n",
    "\n",
    "    res = [ele for ele in Tem if ele != ['Discover your']]\n",
    "\n",
    "    for Company_name_1 in res:\n",
    "        Company_Name.append(Company_name_1[1])\n",
    "\n",
    "    # print(Company_Name)\n",
    "\n",
    "    Experience = soup.find_all('ul', attrs={'class': 'top-jd-dtl clearfix'})\n",
    "\n",
    "    for Exp in Experience:\n",
    "        E = Exp.get_text()\n",
    "\n",
    "        temp1 = E.split(\"card_travel\")\n",
    "        temp2 = temp1[1].splitlines()\n",
    "        Experience_Required.append(temp2[0])\n",
    "\n",
    "    # print(Experience_Required)\n",
    "\n",
    "    Sal = soup.find_all('ul', attrs={'class': 'top-jd-dtl clearfix'})\n",
    "\n",
    "    for S in Sal:\n",
    "        Sa = S.get_text()\n",
    "\n",
    "        temp1 = Sa.split(\"\\n\")\n",
    "        temp2 = temp1[2]\n",
    "        if(temp2.__contains__('â‚¹Rs')):\n",
    "            Salary.append(temp2)\n",
    "        else:\n",
    "            Salary.append(np.nan)\n",
    "\n",
    "    # print(Salary)\n",
    "\n",
    "    Loc = soup.find_all('ul', attrs={'class': 'top-jd-dtl clearfix'})\n",
    "\n",
    "    for L in Loc:\n",
    "        Lo = L.get_text()\n",
    "\n",
    "        temp3 = Lo.splitlines()\n",
    "        if(temp3[4] != 'location_on'):\n",
    "            Location.append(temp3[4])\n",
    "        else:\n",
    "            Location.append(temp3[5])\n",
    "\n",
    "    # print(Location)\n",
    "\n",
    "    Job_Desc = soup.find_all('ul', attrs={'class': 'list-job-dtl clearfix'})\n",
    "\n",
    "    for Desc in Job_Desc:\n",
    "        D = Desc.get_text()\n",
    "\n",
    "        temp1 = D.split(\"\\n\\nJob Description:\\r\\n\")\n",
    "        temp2 = temp1[1].split(\"\\n\\n\\nKeySkills:\\n\\r\\n      \\r\\n          \")\n",
    "\n",
    "        if(temp2[0].__contains__('... More Details')):\n",
    "            temp3 = temp2[0].split(\"... More Details\")\n",
    "            Job_Description.append(temp3[0])\n",
    "        else:\n",
    "            Job_Description.append(temp2[0])\n",
    "\n",
    "    # print(Job_Description)\n",
    "\n",
    "    Job_Desc = soup.find_all('ul', attrs={'class': 'list-job-dtl clearfix'})\n",
    "\n",
    "    for Desc in Job_Desc:\n",
    "        D = Desc.get_text()\n",
    "\n",
    "        temp1 = re.split(\n",
    "            r'\\n\\n\\nKeySkills:\\n\\r\\n      \\r\\n          |\\n\\n\\nKeySkills:|\\r\\n        \\r\\n       \\n\\n', D)\n",
    "        Key_Skills.append(temp1[1])\n",
    "\n",
    "    # print(Key_Skills)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Scraping Shine Web-site</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 26):\n",
    "    url = \"https://www.shine.com/job-search/ai-engineer-jobs-in-india-\" + \\\n",
    "        str(i) + \"?q=AI+Engineer&loc=India\"\n",
    "    sleep(randint(2, 10))\n",
    "\n",
    "    # Making a GET request\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # check status code for response received\n",
    "    # success code - 200\n",
    "    # print(r)\n",
    "\n",
    "    # print content of request\n",
    "    htmlContent = r.content\n",
    "    # print(htmlContent)\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    # print(soup.prettify)\n",
    "\n",
    "    # HTML Tree Traversal\n",
    "\n",
    "    # Get the Title of the HTML page\n",
    "    title = soup.title\n",
    "    # print(title.string)\n",
    "\n",
    "    Job_Data = soup.findAll(\n",
    "        'div', attrs={'class': 'jobCard_jobCard__jjUmu white-box-border'})\n",
    "    # Job_Data\n",
    "\n",
    "    for s in Job_Data:\n",
    "        title = s.h2.text\n",
    "        Job_Title_Name.append(title)\n",
    "\n",
    "        name = s.find('div', class_='jobCard_jobCard_cName__mYnow').text\n",
    "        Company_Name.append(name)\n",
    "\n",
    "        location = s.find(\n",
    "            'div', class_='jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2').text\n",
    "        Location.append(location)\n",
    "\n",
    "        experience = s.find(\n",
    "            'div', class_='jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t').text\n",
    "        Experience_Required.append(experience)\n",
    "\n",
    "        Salary.append(np.nan)\n",
    "\n",
    "        Job_Description.append(np.nan)\n",
    "\n",
    "        Key_Skills.append(np.nan)\n",
    "\n",
    "    # print(Job_Title_Name)\n",
    "    # print(Company_Name)\n",
    "    # print(Location)\n",
    "    # print(Experience_Required)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Scraping Indeed Web-site</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "\n",
    "for i in range(0, 340, 10):\n",
    "    url = \"https://in.indeed.com/jobs?q=AI+Engineer&l=India&start=\" + \\\n",
    "        str(i) + \"&vjk=432eb68d4a23220a\"\n",
    "    sleep(randint(2, 5))\n",
    "\n",
    "    # Making a GET request\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # check status code for response received\n",
    "    # success code - 200\n",
    "    # print(r)\n",
    "\n",
    "    # print content of request\n",
    "    htmlContent = r.content\n",
    "    # print(htmlContent)\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    # print(soup.prettify)\n",
    "\n",
    "    # HTML Tree Traversal\n",
    "\n",
    "    # Get the Title of the HTML page\n",
    "    title = soup.title\n",
    "    # print(title.string)\n",
    "\n",
    "    title = soup.find_all('h2', attrs={'class': 'jobTitle'})\n",
    "\n",
    "    for h2 in title:\n",
    "        A = h2.get_text()\n",
    "        Job_Title_Name.append(A)\n",
    "        x = x + 1\n",
    "    # print(Job_Title_Name)\n",
    "    # print(len(Job_Title_Name))\n",
    "\n",
    "    name = soup.find_all('span', attrs={'class': 'companyName'})\n",
    "\n",
    "    for n in name:\n",
    "        B = n.get_text()\n",
    "        Company_Name.append(B)\n",
    "\n",
    "    # print(Company_Name)\n",
    "    # print(len(Company_Name))\n",
    "\n",
    "    loc = soup.find_all('div', attrs={'class': 'companyLocation'})\n",
    "\n",
    "    for l in loc:\n",
    "        C = l.get_text()\n",
    "        Location.append(C)\n",
    "\n",
    "    # print(Location)\n",
    "    # print(len(Location))\n",
    "\n",
    "    sum = soup.find_all('div', attrs={'class': 'job-snippet'})\n",
    "\n",
    "    for s in sum:\n",
    "        F = s.get_text()\n",
    "        Job_Description.append(F)\n",
    "\n",
    "    # print(Job_Description)\n",
    "    # print(len(Job_Description))\n",
    "\n",
    "for i in range(1, x):\n",
    "    Salary.append(np.nan)\n",
    "    Experience_Required.append(np.nan)\n",
    "    Key_Skills.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sr. No.                            Job Title  \\\n",
      "0        1                         AI Engineer    \n",
      "1        2                         AI Engineer    \n",
      "2        3                         AI Engineer    \n",
      "3        4                         AI Engineer    \n",
      "4        5        Senior Principal AI Engineer    \n",
      "\n",
      "                                Company Name Experience Required Salary  \\\n",
      "0                  Procter & Gamble  ( P&G )           3 - 6 yrs    NaN   \n",
      "1                         Intel Technologies           5 - 8 yrs    NaN   \n",
      "2      ikomet Technology Solutions Pvt. Ltd.           0 - 3 yrs    NaN   \n",
      "3              innovation incubator advisory           0 - 3 yrs    NaN   \n",
      "4             Intel Technology India Pvt Ltd         15 - 18 yrs    NaN   \n",
      "\n",
      "                Location                                    Job Description  \\\n",
      "0                 Mumbai  P & G was founded over 180 years ago as a simp...   \n",
      "1  Bengaluru / Bangalore  AI Engineer Job DescriptionIntel Emergent AI R...   \n",
      "2                Chennai  AI EngineerWork Location: ChennaiRequired Expe...   \n",
      "3     Thiruvananthapuram  Develop custom data models and algorithms to a...   \n",
      "4  Bengaluru / Bangalore  Senior Principal AI Engineer Job DescriptionPr...   \n",
      "\n",
      "                                          Key Skills  \n",
      "0  algorithms  ,  information technology  ,  arti...  \n",
      "1  c    ,  software engineering  ,  image process...  \n",
      "2  algorithms  ,  python  ,  os  ,  machine learn...  \n",
      "3  algorithms  ,  python  ,  problem solving  ,  sql  \n",
      "4  Deep technical  ,  Principal AI Engineer  ,  D...  \n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Sr. No.': [*range(1, len(Job_Title_Name)+1)],\n",
    "    'Job Title': Job_Title_Name,\n",
    "    'Company Name': Company_Name,\n",
    "    'Experience Required': Experience_Required,\n",
    "    'Salary': Salary,\n",
    "    'Location': Location,\n",
    "    'Job Description': Job_Description,\n",
    "    'Key Skills': Key_Skills\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the DataFrame to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Jobs_Website_Dataset.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2caa8a012d727d993b3ae12ee947b2851de7f59fbab49c2d316b966fedb3783"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
